{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch\n",
    "import csv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from load_data import load_preproc_data, load_trte_partition\n",
    "from utils import one_hot_tensor, cal_sample_weight \n",
    "#from models import init_model_dict, init_optim\n",
    "from train_test import prepare_trte_data, gen_trte_adj_mat\n",
    "#from train_test import train_1_epoch, test_VCDN\n",
    "import copy\n",
    "\n",
    "from utils import print_dict\n",
    "from temporaries import findInteractionsSelf, findInteractionsCross, label_specific_acc\n",
    "\n",
    "import cufflinks as cf\n",
    "import pathlib\n",
    "%matplotlib inline\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the all charts as public\n",
    "cf.set_config_file(sharing='public',theme='pearl',offline=False)\n",
    "cf.go_offline()\n",
    "\n",
    "cwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED can be \"random\" or integer, if integer, it will be used as the seed for random, numpy, torch, and cuda\n",
    "SEED = \"random\" \n",
    "\n",
    "# pre-processed data\n",
    "mrna = \"/data/users/bs16b001/R/TCGA BRCA/mrna_top1000.csv\"\n",
    "meth = \"/data/users/bs16b001/R/TCGA BRCA/meth_top1000.csv\"\n",
    "mirna = \"/data/users/bs16b001/R/TCGA BRCA/mirna_anova.csv\"\n",
    "meta_csv = \"/data/users/bs16b001/R/TCGA BRCA/PAM50_subtype.csv\"\n",
    "trte_partition_file = \"/data/users/bs16b001/R/TCGA BRCA/trte_partition.txt\"\n",
    "\n",
    "# change label from text to integer\n",
    "label_dict = {'Normal':0, 'Basal':1, 'Her2':2, 'LumA':3, 'LumB':4}\n",
    "\n",
    "# load preprocessed data from csv\n",
    "#load_list - list of csv files to laod. The -2 position should be meta_csv and -1 position should be trte_partition_file\n",
    "load_list = [mrna, meth, mirna, meta_csv, trte_partition_file]\n",
    "GCN_names = [\"mRNA\",\"methylation\",\"miRNA\"]\n",
    "\n",
    "doSMOTE = True # Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign GPU number\n",
    "CUDA_DEVICE = 0\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "if cuda:\n",
    "    Print(\"CUDA Device in use\")\n",
    "\n",
    "# Set random seed\n",
    "if SEED == \"random\":\n",
    "    SEED = random.randint(0,100000)\n",
    "print(\"SEED = \", SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.set_device(CUDA_DEVICE)\n",
    "\n",
    "# load preprocessed data from csv\n",
    "\n",
    "num_class = len(label_dict)\n",
    "data_list, labels, patient_id, feat_name_list = load_preproc_data(load_list[:-2], load_list[-2], label_dict)\n",
    "# load tr/te partition\n",
    "tr_idx, te_idx = load_trte_partition(load_list[-1], patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare data and model for training and testing\n",
    "\"\"\"\n",
    "# data to tensor\n",
    "data_tensor_list = []\n",
    "for i in range(len(data_list)):\n",
    "    data_tensor_list.append(torch.FloatTensor(data_list[i]))\n",
    "    if cuda:\n",
    "        data_tensor_list[i] = data_tensor_list[i].cuda()\n",
    "\n",
    "# generate training and testing data\n",
    "data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_tensor_list, labels, tr_idx, te_idx)\n",
    "\n",
    "\n",
    "labels_tr_tensor = torch.LongTensor(labels_trte[trte_idx[\"tr\"]])\n",
    "\n",
    "# Here we modify the training data set using SMOTE. We have only changed the training data.\n",
    "# While testing, we use training data, but only the real ones, not the synthesised ones\n",
    "data_tr_list_nosmote = copy.deepcopy(data_tr_list)\n",
    "if doSMOTE:\n",
    "    sm = SMOTE(random_state = SEED)\n",
    "    for i in range(len(data_tr_list)):\n",
    "        data_tr_list[i], labels = sm.fit_sample(data_tr_list[i], labels_tr_tensor)\n",
    "        data_tr_list[i] = torch.FloatTensor(data_tr_list[i])\n",
    "    labels_tr_tensor = torch.LongTensor(labels)\n",
    "    onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
    "\n",
    "    sample_weight_tr = cal_sample_weight(labels_tr_tensor.numpy(), num_class)\n",
    "    sample_weight_tr =  torch.FloatTensor(sample_weight_tr)\n",
    "else:\n",
    "    onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
    "    sample_weight_tr = cal_sample_weight(labels_trte[trte_idx[\"tr\"]], num_class)\n",
    "    sample_weight_tr =  torch.FloatTensor(sample_weight_tr)\n",
    "\n",
    "if cuda:\n",
    "    labels_tr_tensor = labels_tr_tensor.cuda()\n",
    "    onehot_labels_tr_tensor = onehot_labels_tr_tensor.cuda()\n",
    "    sample_weight_tr = sample_weight_tr.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some necessary lists\n",
    "ground_truth_test = labels_trte[trte_idx[\"te\"]]\n",
    "ground_truth_train = labels_tr_tensor.numpy()\n",
    "temp_df = pd.DataFrame(ground_truth_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "I'll try out svm from the sklearn package here and then extract features from it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = np.array(torch.cat(tuple(each for each in data_tr_list), dim=1))\n",
    "feature_names = [name for sublist in feat_name_list for name in sublist]\n",
    "Y = labels_tr_tensor.numpy()\n",
    "\n",
    "classifier = svm.SVC(probability=True)\n",
    "classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = classifier.predict(X)\n",
    "f1_score(Y, train_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(torch.cat(tuple(each for each in data_trte_list), dim=1))[trte_idx[\"te\"]] #Only test samples\n",
    "test_pred = classifier.predict(X_test)\n",
    "f1_score(ground_truth_test, test_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME\n",
    "Feature Importance Measurement using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIME\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X, feature_names=feature_names, \n",
    "                                                   class_names=label_dict.values(), discretize_continuous=True)\n",
    "\n",
    "data_full = np.array(torch.cat(tuple(each for each in data_trte_list), dim=1))\n",
    "sp_obj = submodular_pick.SubmodularPick(explainer, data_full, classifier.predict_proba, method=\"full\", num_features=min(2000,data_full.shape[1]), num_exps_desired=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pd.DataFrame([dict(each.as_list(each.available_labels()[0])) for each in sp_obj.explanations]).fillna(0)\n",
    "W['prediction'] = [each.available_labels()[0] for each in sp_obj.explanations]\n",
    "\n",
    "importance = W.groupby(\"prediction\").mean().T   #Mean of lime coefficients by class\n",
    "importance[\"aggregate\"] = abs(importance).sum(axis=1) #Calculating aggregate importance\n",
    "\n",
    "new_index = [re.search(\"(.*)[\\>\\<]\\d*\", name).group(1).strip().split(' ')[-1] for name in importance.index]\n",
    "\n",
    "importance = importance.rename(index=dict(zip(importance.index,new_index)))[\"aggregate\"]\n",
    "importance = importance.groupby(level=0).sum()\n",
    "importance.to_csv(\"svm_lime.csv\", index_label=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_lime_score = np.mean([each.score for each in sp_obj.explanations])\n",
    "std_dev = np.std([each.score for each in sp_obj.explanations])\n",
    "print(average_lime_score, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the aggregate importances\n",
    "np.abs(W.drop(\"prediction\", axis=1)).mean(axis=0).sort_values(ascending=False).head(25).sort_values(\n",
    "    ascending=True).iplot(kind=\"barh\")\n",
    " \n",
    "#Aggregate importances split by classes\n",
    "grped_coeff = W.groupby(\"prediction\").mean()\n",
    " \n",
    "grped_coeff = grped_coeff.T\n",
    "#grped_coeff[\"abs\"] = np.abs(grped_coeff.iloc[:, 0])\n",
    "grped_coeff[\"abs\"] = abs(grped_coeff).sum(axis=1)\n",
    "grped_coeff.sort_values(\"abs\", inplace=True, ascending=False)\n",
    "grped_coeff.head(25).sort_values(\"abs\", ascending=True).drop(\"abs\", axis=1).iplot(kind=\"barh\", bargap=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "Feature Importance Measurement using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "import shap\n",
    "\n",
    "train_data_full = np.array(torch.cat(tuple(each for each in data_tr_list), dim=1))\n",
    "kmeans_data = shap.kmeans(train_data_full, k=5)\n",
    "explainer = shap.KernelExplainer(classifier.predict_proba, kmeans_data)\n",
    "data_test = np.array(torch.cat(tuple(each for each in data_trte_list), dim=1))\n",
    "shap_values = explainer.shap_values(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [name for sublist in feat_name_list for name in sublist]\n",
    "shap.summary_plot(shap_values = shap_values, feature_names=feature_names, max_display=40, plot_type='bar', class_names=list(label_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_list = np.sum(np.array([np.mean(abs(each), axis=0) for each in shap_values]), axis=0)\n",
    "shap_df = pd.DataFrame({\"features\":feature_names, \"shapley_values\":shap_list})\n",
    "shap_df.sort_values(by=\"features\", ascending=True).set_index(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_shapley_values = np.array(shap_values)\n",
    "classes = label_dict.keys()\n",
    "samples = patient_id\n",
    "features = feature_names\n",
    "names = ['classes', 'samples', 'features']\n",
    "index = pd.MultiIndex.from_product([range(s)for s in raw_shapley_values.shape], names=names)\n",
    "#index.set_names(names, inplace=True)\n",
    "raw_shapley_values = pd.DataFrame({'Shap_values': raw_shapley_values.flatten()}, index=index)['Shap_values']\n",
    "raw_shapley_values.index = pd.MultiIndex.from_tuples([(x,y,z) for x in classes for y in samples for z in features])\n",
    "time = str(datetime.datetime.now())\n",
    "raw_shapley_values.to_csv(\"raw_shapley_values_\"+time+\".csv\", index_label=names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
